# scalar-sparse-ai

Implementation of Scalar-Sparseâ„¢ architecture for hyper-context AI - a radical compression approach to enable massive context windows on consumer hardware.

## ðŸš€ Quick Start

### Prerequisites

- Python 3.10+
- [uv](https://github.com/astral-sh/uv) for fast Python package management

### Installation

```bash
# Install uv if you haven't already
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone the repository
git clone https://github.com/username/scalar-sparse-ai.git
cd scalar-sparse-ai

# Create virtual environment and install dependencies
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install the project
uv pip install -e .

# Install development dependencies
uv pip install -e ".[dev]"
```

### Running Experiments

```bash
# Run the basic proof of concept
python -m scalar_sparse_ai.scalar_sparse_poc

# Run sparse attention demonstration
python -m scalar_sparse_ai.sparse_attention_demo

# Run the original proof of concept
python -m scalar_sparse_ai.proof_of_concept

# Use the CLI
scalar-sparse compress --dimensions 1 4 8 16 32 64
scalar-sparse memory

# Run Colab-ready experiments locally
python colab_experiments.py
```

## ðŸ“Š Key Concepts

Based on research showing:
1. **95% Parameter Reduction**: Attention heads can be approximated by simple MLPs
2. **5% Token Importance**: Only 5-10% of token interactions are meaningful
3. **Massive Compression**: <10 dimensions per token enables 3M+ token contexts

## ðŸ§ª Experiments

### 1. Compression Level Testing
Tests different dimension sizes (1-256) to find the "phase transition" point where coherent behavior emerges.

### 2. Sparse Attention
Demonstrates that attention mechanisms contain massive redundancy, with most computation being noise.

### 3. Memory Efficiency
Shows how Scalar-Sparse representation enables:
- 8K tokens â†’ 40KB (fits in L3 cache)
- 100K tokens â†’ 488KB (laptop RAM)
- 1M tokens â†’ 4.8MB (consumer GPU)
- 3.2M tokens â†’ 15.6MB (still fits in GPU!)

## ðŸ› ï¸ Development

```bash
# Run tests
pytest

# Format code
black scalar_sparse_ai/
ruff check scalar_sparse_ai/

# Type checking
mypy scalar_sparse_ai/

# Start Jupyter for interactive experiments
jupyter notebook
```

## ðŸ“ Project Structure

```
scalar-sparse-ai/
â”œâ”€â”€ scalar_sparse_ai/             # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py                    # Command line interface
â”‚   â”œâ”€â”€ scalar_sparse_poc.py      # Main experiments
â”‚   â”œâ”€â”€ sparse_attention_demo.py  # Sparse attention experiments
â”‚   â””â”€â”€ proof_of_concept.py       # Original implementation
â”œâ”€â”€ colab_experiments.py          # Google Colab ready script
â”œâ”€â”€ pyproject.toml               # Project configuration
â”œâ”€â”€ .gitignore                   # Git ignore rules
â””â”€â”€ README.md                    # This file
```

## ðŸ”¬ Research Papers

This implementation is based on:
1. "Attention Heads Can Be Approximated by Simple Neural Networks" (Bee, 2025)
2. "Sparse MLP Approximation of Attention" (Bee, 2025)
3. "From Redundancy to Revolution: A Scalar-Sparse Architecture" (Bee, 2025)

## ðŸš€ Next Steps

- [ ] Full implementation of Scalar-Sparse encoder/decoder
- [ ] Benchmark on downstream tasks
- [ ] Build 1M+ token context demonstration
- [ ] Optimize with CUDA kernels for production speed

## ðŸ“„ License

MIT License - see LICENSE file for details.
